\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage[top=2cm, bottom=2cm, left=2cm, right=2cm]{geometry}
\setlength\parindent{0pt}

\usepackage{listings}
%\usepackage{color}
\usepackage{graphicx}
\usepackage{float}
\usepackage{caption}

\usepackage{verbatim}
\let\oldv\verbatim
\let\oldendv\endverbatim

%\userpackage{minted}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\definecolor{light-gray}{gray}{0.95}


\lstset{frame=tb,
  language=Java,
  aboveskip=6mm,
  belowskip=6mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3,
  backgroundcolor=\color{light-gray},
  language=Matlab
}

%\usepackage{natbib} replaced by line below to make refernces work
\usepackage[square,sort,comma,numbers]{natbib}
\usepackage[nottoc,numbib]{tocbibind} %to get references in table of contants
\usepackage{graphicx}

\usepackage{bm}

\usepackage{hyperref}
\hypersetup{
	colorlinks,
	citecolor=black,
	filecolor=black,
	linkcolor=black,
	urlcolor=black
}

\usepackage{mdframed}
\usepackage{lipsum} % for creating dummy text
\mdfdefinestyle{MyFrame}{%
	linecolor=black,	
	backgroundcolor=gray!20!white,
	skipbelow = 8mm,
	skipabove = 8mm}

\usepackage{scrextend}

\usepackage{multimedia}
\usepackage{media9}

\usepackage{booktabs}
\usepackage{adjustbox}

\title{Fys4150\\Project 5\\ }
\author{Peter Killingstad and Karl Jacobsen\\
\\
\url{https://github.com/kaaja/fys4150}}

\begin{document}
	
\maketitle

\section*{Note to instructurs reagarding Github repository}
If the above Github-link does not work, it is eighter because you have not yet accepted our invite to the repository, or you have not yet provided us with an e-mail addres available at Github so that we can invite you. The Github user you will be invited from is "kaaja". If the latter applies to you, please send us an e-mail with an e-mailadress available in Github or your Github username so that we can send you an invite. Our e-mailadresses: peter.killingstad@hotmail.com, karljaco@gmail.com.


\section{Analytical solution 1D}
The one-dimensional problem is written as 

\begin{subequations}
	\begin{eqnarray}
	\frac{\partial^2 u(x,t)}{\partial x^2} &=& \frac{\partial u(x,t)}{\partial t} \textit{, } t> 0 \textit{, } x \in (0,L) \\ \nonumber
	\\
	u(x,0) &=& 0 \textit{, } 0 <x < 1 \textit{,} \\
	u(0,t) &=& 0 \textit{, } t>0 \\
	u(L,t) &=& 1 \textit{, } t>0,
	\end{eqnarray}
\end{subequations}

where the length has been scaled by the lenght of the $x$-domain, $L$.\\

The problem consists of non-homogeneous boundary conditions, and it is not trivial to find a closed form solution. The problem can be seen as a physical problem such as a temperature gradient in a rod or flow between two infinite flat plates, where the fluid is initially at rest and the plate at $x=1$ is given a sudden movement. \\

From physical observations we know that as time goes, i.e. when $t\rightarrow \infty$,  the problem coinsides whith its surroundings and becomes steady.It is reasonable to introduce a solution consisting of the sum of two parts, a steady-state solution, and a transient solution that depends on the initial conditions:

\begin{equation}
u(x,t) = U(x) + V(x,t)
\end{equation}
where $U(x)$ is the steady-state solution and $V(x,t)$ is the transient solution.\\ 

Since the steady-state problem is not changing in time, i.e. $\partial_ t = 0$, it is written in a compact form as:
\begin{subequations}
	\begin{eqnarray}
	\label{eqn:steadyStateSODE}
	U_{xx} &=& 0 \textit{ , } x \in [0,1]\\ \nonumber
	\\
	U(0) &=& 0 \textit{  } \\
	U(L) &=& 1
	\end{eqnarray}
\end{subequations}

For the transient part of the problem we have the problem written in compact form as:

\begin{subequations}
	\begin{eqnarray}
	\label{eqn:transientODE}
	V_t &=& V_{xx} \textit{ , } t>0 \textit{ , } x \in (0,1) \\ \nonumber
	\\
	\label{eqn:transientIC}
	V(x,0) &=& -U(x) \textit{ , } 0<x<1 \\
	V(0,t) &=& 0 \textit{ , } t>0 \\
	V(1,t) &=& 0 \textit{ , } t>0
	\end{eqnarray}
\end{subequations}


The steady state solution is solved by integrating (\ref{eqn:steadyStateSODE}) twice

\begin{eqnarray}
\nonumber
\int \frac{\partial^2 u(x,t)}{\partial x^2}\;dx &=& A \\ \nonumber
\int \frac{\partial u(x,t)}{\partial x}\;dx &=& \int Adx +B \\ \nonumber
\end{eqnarray}

and we end up with the general solution $U(x) = Ax + B$. Applying the boundary conditions at $x=0$ and $x=1$ we get that 

\begin{equation}
U(x) = x
\label{eqn:UsteadyState1D}
\end{equation}

The transient problem (\ref{eqn:transientODE}) has homogeneous boundary conditions, and we can solve it by seperation of variables. We start by making the anzats that $V(x,t) = T(t)X(x)$ so that we can rewrite (\ref{eqn:transientODE}) as:

\begin{equation}
XT'=X''T 
\end{equation}

Reordering the equation we get the following:

\begin{equation}
\frac{T'}{T} = \frac{X''}{X} = k,
\end{equation}
where $k$ is an unknown constant. The reason we can insert $k$ into the above equation, is that the sides depends on different varirables, and so equality between the sides is only acheived if both sides equals a constant. We have homogenuous BCs, and for this to be achieved when we solve the $X$-part of the above equaion, we must have $k <0$. As can be seen later, with $k\geq 0$ we would not be able to satisfy the homogenupus BCs. For computationaly ease, we set $k = -\lambda^2$, where $\lambda > 0$. \\

Starting with the $t$ dependent part, we get

\begin{eqnarray}
\frac{T'}{T} = -\lambda^2 \\ 
T' = -\lambda^2 T
\end{eqnarray}

The ODE can be solved by separation of variables

\begin{subequations}
	\begin{eqnarray}
	\label{eqn:solvingTdependentTransientTerm1D}
	\frac{1}{T}\frac{dT}{dt} = -\lambda^2 \\ 
	\Rightarrow \frac{1}{T} dT = -\lambda^2 dt\\
	\int \frac{1}{T}dT = - \int \lambda^2 dt \\
	\Rightarrow \ln(T) = -\lambda^2 + A
	\end{eqnarray}
\end{subequations}

The $t$ dependent term is then given by

\begin{equation}
T = Ae^{-\lambda^2 t}
\label{eqn:tdependentTransientTerm1D}
\end{equation}

We see that our assumption $\lambda > 0$ makes sense also for the $T$-solution, since we do not get blow-up as $t \rightarrow \infty$. \\

For the $x$ dependent term we get

\begin{subequations}
	\begin{eqnarray}
	X'' = -\lambda^2 X \\
	\Rightarrow X'' + \lambda^2 X = 0 
	\end{eqnarray}
\end{subequations}

We use an anzats $X=e^{\alpha x}$:

\begin{subequations}
	\begin{eqnarray}
	\alpha^2 e^{\alpha x} + \lambda^2 e^{\alpha x} &=& 0 \\ 
	\Rightarrow \alpha^2 &=& -\lambda^2 \\ 
	\Rightarrow \alpha &=& \pm \sqrt{-\lambda^2}
	\end{eqnarray}
\end{subequations}

Since we have $\lambda > 0$, we get our wanted trigonometric solution

\begin{equation}
X = Be^{-i\lambda x} + Ce^{i\lambda x},
\label{eqn:xDependenttransientTerm1D}
\end{equation}

which can be written as

\begin{equation}
X(x) = B\cos(\lambda x) + C\sin(\lambda x) 
\end{equation},

where the constants have changed.\\

Applying the boundary conditions we get that

\begin{eqnarray}
X(0) = B\cos(0) + C\sin(0) = 0 \\ 
X(1) = B\cos(\lambda) + C\sin(\lambda) = 0
\end{eqnarray}

From the first boundary condition wher $x=0$ we must have that $B=0$. We are lookong for non-trivial solutions of the problem, so for $x=1$ we must find the  values that gives $C\sin(\lambda)=0$. We know that\\ $\sin(n \pi) =0 \textit{ for } n = 1,2, \ldots $, which lets us determine that $\lambda = n \pi$.\\

Summing up, we have the following particular solutions for the transient part:

\begin{subequations}
	\begin{eqnarray}
	V_n(x,t) = A_ne^{-(n\pi)^2 t}\sin(n \pi x), n=1,2,...,\infty
	\end{eqnarray}
\end{subequations}

Since all particular solutions satisfy the PDE, and the BCs are homogenuous, all linear combinations of the particular solutions will also satisfy our problem, giving the general soultion of the transient problem

\begin{equation}
\sum_{n=1}^{\infty} a_ne^{-(n\pi)^2 t}\sin(n \pi x)
\label{eqn:FundamentalSolutionODE}
\end{equation}

The coefficients $a_n$ is found by applying the initial condidtion (IC)

\begin{equation}\label{eq:genSolTrans}
-U(x) = \sum_{n=1}^{\infty} a_n\sin(n \pi x) 
\end{equation}

To obtain an expression for the coefficients $a_n$ we use that the functions $\sin(n\pi x)$ is orthogonal to each other in the sense that 

\begin{equation}
\int_0^1 \sin(n\pi x) \sin(m\pi x) dx = 
\begin{cases} 0 & \quad \text{if } m \neq n\\
1/2 & \quad \text{if } m = n 
\end{cases}
\label{eqn:orhogonalSine}
\end{equation}

Using the above statement, multiplying \ref{eq:genSolTrans} by $\sin(m\pi x)$  and then integrating over $x$ over the domain, we get 

\begin{eqnarray}
\nonumber
- \int_0^1 U(x)\sin(m\pi x)\;dx &=& a_n \int_0^1 \sum_{n=1}^{\infty} \sin(m\pi x)\sin(n\pi x)\;dx = a_m\int_0^1 \sin^2(m\pi x)\;dx\\ \nonumber
&\Rightarrow& - \int_0^1 U(x)\sin(m\pi x) =\frac{a_m}{2}\\ 
&\Rightarrow& a_m = - 2\int_0^1 U(x)\sin(m\pi x) \;dx
\end{eqnarray}

Solving for $a_n$, applying the steady state solution (\ref{eqn:UsteadyState1D}), yields 

\begin{eqnarray}
\nonumber
a_n = -2 \int_0^1 x \sin(m\pi x) &=& \Big[\frac{2}{(m\pi)^2} \sin(m\pi x) + \frac{2x}{m\pi}\cos(m\pi x)\Big]_0^1 \\ 
\label{eqn:fourierCoefficients}
&\Rightarrow & a_n = \frac{2}{m\pi}(-1)^m
\end{eqnarray}
\newline

Putting the coefficient from (\ref{eqn:fourierCoefficients}) into the fundamental solutions (\ref{eqn:FundamentalSolutionODE}) we end up with the following solution for the transient problem:

\begin{equation}
V(x,t) = 2\sum_{n=1}^{\infty} \frac{(-1)^k}{n\pi} e^{-(n\pi)^2 t}\sin(n\pi x)
\label{eqn:transientSolution1D}
\end{equation}

Now we get an expression for the whole problem by putting combining the steady-state solution (\ref{eqn:UsteadyState1D}) and the transient solution (\ref{eqn:transientSolution1D}) such that we obtain 

\begin{equation}
u(x,t) = U(x) + V(x,t) = x + 2\sum_{n=1}^{\infty}e^{-(n\pi)^2 t} \frac{(-1)^k}{n\pi}
\sin(n\pi x)
\label{eqn:solution1D}
\end{equation}

\section{Analytical solution 2D}
In the two-dimensional case the differential equation becomes
\begin{subequations}
	\begin{eqnarray}
	\frac{\partial^2 u(x,y,t)}{\partial x^2} + \frac{\partial^2 u(x,y,t)}{\partial y^2} &=&  \frac{\partial u(x,y,t)}{\partial t} \textit{ , } t>0 \textit{ , } x,y \in (0,1) \\ \nonumber \\
	u(x,y,0) &=& 0 \textit{ , } x,y \in (0,1) \\ 
	u(0,y,t) &=& 0 \textit{ , } t> 0 \\
	u(1,y,t) &=& 0 \textit{ , } t> 0 \\
	u(x,0,t) &=& 0 \textit{ , } t> 0  \\
	u(x,1,t) &=& 1 \textit{ , } t> 0 
	\end{eqnarray}
\end{subequations}

The boundary conditions is extended in such a way that the physical problem can be that of a flow whitin a box with infinite plates in the streamwise direction ($z$), where the fluid is initially at rest and the plate at $y=1$ is given a sudden movement. We have "no slip" boundary conditions i.e. the fluid has zero movement on all boundaries except at $y=1$.\\

To obtain a closed form solution for the two-dimensional, we use the same argument as for the one-dimensional problem, i.e. we split the solution into a steady-state solution and a transient solution.\\

We end up with a solution defined as:
\begin{equation}
u(x,y,t) = U(x,y) + V(x,y,t)
\end{equation}
where $U(x,y)$ is the steady-state solution and $V(x,y,t)$ is the transient solution.\\

The steady-solution is then written in a compact form as:
\begin{subequations}
	\begin{eqnarray}
	U_{xx} + U_{yy} &=& 0 \textit{ , } x,y \in [0,1]\\ \nonumber
	\\
	U(0,y) &=& 0 \textit{  } \\
	U(1,y) &=& 0 \textit{  } \\
	U(x,0) &=& 0 \textit{  } \\
	U(x,1) &=& 1 \textit{  } 
	\end{eqnarray}
\end{subequations}

For the transient problem we have the propblem defined as:

\begin{subequations}
	\begin{eqnarray}
	\label{eqn:transientPDE}
	V_t &=& V_{xx} + V_{yy} \textit{ , } t>0 \textit{ , } x \in [0,1] \\ \nonumber
	\\
	\label{eqn:transientICPDE}
	V(x,0) &=& -U(x,y) \textit{ , } x,y \in [0,1] \\
	V(0,y,t) &=& 0 \textit{ , } t>0 \\
	V(1,y,t) &=& 0 \textit{ , } t>0 \\
	V(x,0,t) &=& 0 \textit{ , } t>0 \\
	V(x,1,t) &=& 0 \textit{ , } t>0 
	\end{eqnarray}
\end{subequations}

For the steady-state problem we have a 2D Laplacian equation, which we can solve by seperation of variables. We use the anzats that $U(x,y) = X(x)Y(y)$, so that we end up with:

\begin{eqnarray}
\nonumber
YX'' + XY''=0 \\ \nonumber
\frac{X''}{X} = - \frac{Y''}{Y} = -\beta^2
\end{eqnarray}

Due to the independence of the terms on each side of the equation, we can solve the equations separately, each equation becoming

\begin{equation}
\frac{X''}{X} = -\beta^2 \nonumber
\end{equation}

\begin{equation}
\frac{Y''}{Y} = \beta^2 \nonumber
\end{equation}

The sign on the right hand side in both of the above equations is determined by the fact that in the $x$ dependent term we have homogeneous boundaries, while in the $y$ dependent term we have non-homogeneous. By the same reasoning as in the 1D-case, homogenupus BCs in the $x$-direction, we get $\beta > 0$. \\

For the $x$ dependent term we get

\begin{eqnarray}
\nonumber
X'' = -\beta^2 X \\ \nonumber
\Rightarrow X'' + \beta^2 X = 0,
\end{eqnarray}

which has a similar solution as that of the transient $x$ dependent term in the one-dimensional case (\ref{eqn:xDependenttransientTerm1D}). With our choise of $\beta$, we get the wanted trigonometric form

\begin{equation}
X(x) = A\cos(\beta x) + B\sin(\beta x) \nonumber.
\end{equation}

Applying the boundary conditions, we end up with the term

\begin{equation}
X_n(x) = B_n\sin(n\pi x),
\label{eqn:xDependenSteadyState2D}
\end{equation}

were ${X_n(x)}$ is the family of particular solutions.\\

The $y$ dependent term can be rewritten as 

\begin{subequations}
	\begin{eqnarray}
	y'' = \beta^2 Y \\ 
	\Rightarrow Y'' - \beta^2 Y = 0 
	\end{eqnarray}
\end{subequations}

We are again using an anzats so that $Y = e^{\gamma y}$. This leaves us with the term

\begin{subequations}
	\begin{eqnarray}
	\gamma^2 e^{\gamma y} - \beta^2 e^{\gamma y} &=& 0 \\ 
	\Rightarrow \gamma^2 &=& \beta^2 \\ 
	\Rightarrow \gamma &=& \pm \beta,
	\end{eqnarray}
\end{subequations}

and we end up with the following expression

\begin{eqnarray}
Y_n(y) &=& C_ne^{-n\pi y} + D_ne^{n\pi y} \\ 
&\Rightarrow & C_n\cosh(n\pi y) + D_n\sinh(n\pi y)
\end{eqnarray}
for the family of particular solutions.
\\

The fundamental solutions of the steady-state is the given by
\begin{equation}
U(x,y) = \sum_{n=1}^{\infty} X(x)Y(y) = \sum_{n=1}^{\infty} B_n\sin(n\pi x)\Big(C_n\cosh(n\pi y) + D_n\sinh(n\pi y)\Big)
\end{equation}

Applying the the boundary of $U(x,0)$ to the equation above we get that $C_n$ must be zero. We are then left with the expression

\begin{equation}
U(x,y) = \sum_{n=1}^{\infty} c_n \sin(n\pi x)\sinh(n\pi y)
\label{eqn:UsteadyStatefundamentalSolutions}
\end{equation}

Applying the last boundary $U(x,1) = 1$ we must have that

\begin{equation}
1 = \sum_{n=1}^{\infty} c_n \sin(n\pi x)\sinh(n\pi)
\end{equation}

setting $d_n = c_n\sinh(n\pi)$ we get that 

\begin{equation}
1 = \sum_{n=1}^{\infty} d_n \sin(n\pi x)
\end{equation}

Using again that the functions $\sin(n\pi x)$ is orthogonal (\ref{eqn:orhogonalSine}), as in the 1D case, multiplying the above equation with $ \sin(m\pi x)$ and  then integrating over $x$ over the domain, we get that 

\begin{subequations}
	\begin{eqnarray}
	d_n = 2\int_0^1 \sin(m\pi x) dx = \frac{2}{m\pi}\Big(1-(-1)^m\Big)\\
	\Rightarrow c_n = \frac{2}{m\pi \sinh(m\pi)}\Big(1-(-1)^m\Big).
	\end{eqnarray}
\end{subequations}

Putting the coefficients $c_n$ back into the expression for the fundamental solutions (\ref{eqn:UsteadyStatefundamentalSolutions}), we end up with the following expression for the steady-state
\begin{subequations}
	\begin{eqnarray}
	U(x,y) &=& \frac{2}{\pi} \sum_{m=1}^{\infty} \frac{\sin(m\pi x)\sinh(m\pi y)}{\pi \sinh(m\pi)}\Big(1-(-1)^m \Big)\\ 
	&\Rightarrow & U(x,y) = \frac{4}{\pi} \sum_{m=1}^{\infty} \frac{\sin((2m-1)\pi x)\sinh((2m-1)\pi y)}{\pi \sinh((2m-1)\pi)}.
	\end{eqnarray}
\end{subequations}

For the transient problem we once again use the anzats that $V(x,y,t) = X(x)Y(y)T(t)$ in order to write the problem as

\begin{subequations}
	\begin{eqnarray}
	XYT' = YTX'' + XTY'' \\
	\Rightarrow \frac{T'}{T} = \frac{X''}{X} + \frac{Y''}{Y} = -k^2.
	\end{eqnarray}
\end{subequations}

Starting by solving the $t$ dependent term, which is solved in the same way as for the one-dimensional term (\ref{eqn:solvingTdependentTransientTerm1D}), we end up with the expression

\begin{equation}
T=Ae^{-k^2t}.
\label{eqn:2DtransientTdependentGeneralTerm}
\end{equation}

For the $x$ dependent part we have 

\begin{subequations}
	\begin{eqnarray}\label{eq:xOde}
	\frac{X''}{X} = -\frac{Y''}{Y} - k^2\\
	\Rightarrow \frac{X''}{X} = -\gamma^2,
	\end{eqnarray}
\end{subequations}

where 

\begin{equation}\label{eq:gamma}
	-\gamma^2 = \frac{Y^{''}}{Y} - k^2.
\end{equation}

(\ref{eq:xOde}) has the same solution form as the $x$ dependent transient one-dimensional term (\ref{eqn:xDependenttransientTerm1D}), as well as the steady state two-dimensional term (\ref{eqn:xDependenSteadyState2D}), so we end up with 

\begin{equation}
X(x) = B\cos(\gamma x) + C\sin(\gamma x)
\end{equation}

For the above equation to hold on the boundaries, we get  $B=0$ and $\gamma = n\pi, n=1,2,...,\infty$, so we end up with

\begin{equation}
X_n(x) = c_n\sin(n\pi x)
\label{eqn:2DtransientXdependentTerm}
\end{equation}

We are left with the $y$ dependent term, which is written as 

\begin{subequations}
	\begin{equation}
	\frac{Y''}{Y} = k^2 - \gamma^2\\
	\end{equation}
\end{subequations}

Solving for $Y$ we again use the anzats $Y=e^{\alpha y}$ and obtain the term

\begin{subequations}
	\begin{eqnarray}
	\alpha^2 e^{\alpha y} + (\gamma^2 - k^2)e^{\alpha y} = 0 \\
	\alpha^2 + \gamma^2 - k^2 = 0 \\
	\Rightarrow \alpha = \pm \sqrt{k^2 - \gamma^2} 
	\end{eqnarray}
\end{subequations}

We are looking for a solution on the form that satisfies the homogenupus BCs for the transient term, resulting in

\begin{equation}
Y = D\cos(\sqrt{k^2 - \gamma^2}y) + E\sin(\sqrt{k^2 - \gamma^2}y)
\end{equation}

For the above equation to satisfy the boundary conditions $Y(0) = 0$ and $Y(1) = 0$, we get that $D = 0$ and that $\sqrt{k^2 - \gamma^2} = m\pi,\; m = 1,2,...\infty$. The $Y$ term is then given by

\begin{equation}
Y_m(y) = E_m\sin(m\pi y)
\label{eqn:2DtransientYdependentTerm}
\end{equation}

We can now calculate the constant $k^2$,  $k^2 = \gamma^2 + (m\pi)^2 = (n\pi)^2 + (m\pi)^2$. Putting this back into the $t$ dependent transient term (\ref{eqn:2DtransientTdependentGeneralTerm}) we have that 

\begin{equation}
T = A_{nm}e^{-\pi^2(m^2+n^2)t}.
\label{eqn:2DtransientTdependentTerm}
\end{equation}

Combining the $x,y$ and $t$ dependent terms ((\ref{eqn:2DtransientXdependentTerm}), (\ref{eqn:2DtransientYdependentTerm}) and (\ref{eqn:2DtransientTdependentTerm}) respectively), we end up with the fundamental solution

\begin{equation}
V(x,y,t) = \sum_{n=1}^{\infty} \sum_{m=1}^{\infty} A_{mn}\sin(n\pi x)\sin(m\pi y)e^{-\pi^2(m^2+n^2)t}.
\end{equation}

To find the coefficients $A_{nm}$, we use the initial condition

\begin{equation}
-U(x,y) =  \sum_{n=1}^{\infty} \sum_{m=1}^{\infty} A_{mn}\sin(n\pi x)\sin(m\pi y)
\end{equation}

We can solve for the coefficient by again using the orthogonality from (\ref{eqn:orhogonalSine}), multiplying the equation above by $ \sin(p\pi x) \sin(q\pi y)$, and integrating over the $x$ and $y$ domain to get

\begin{subequations}
	\begin{eqnarray}
	\int_0^1 \int_0^1 -U(x,y) \sin(p\pi x)\sin(q\pi y) dxdy =  \frac{A_{pq}}{4}\\
	\Rightarrow A_{pq} = -4 \int_0^1 \int_0^1 U(x,y) \sin(p\pi x)\sin(qn\pi y)\; dxdy.
	\end{eqnarray}
\end{subequations}

Finally we can combine the steady-state solution and the transient solution in order to get the analytical expression

\begin{eqnarray}
u(x,y,t) &=& U(x,y) + V(x,y,t)\\ \nonumber
&=& U(x,y) + \sum_{n=1}^{\infty} \sum_{m=1}^{\infty} A_{mn}\sin(n\pi x)\sin(m\pi y)e^{-\pi^2(m^2+n^2)t},
\end{eqnarray}

where

\begin{equation}
U(x,y) = \frac{4}{\pi} \sum_{m=1}^{\infty} \frac{\sin((2m-1)\pi x)\sinh((2m-1)\pi y)}{\pi \sinh((2m-1)\pi)}
\end{equation}

and 

\begin{equation}
A_{mn} = -4 \int_0^1 \int_0^1 U(x,y) \sin(m\pi x)\sin(n\pi y) dxdy.
\end{equation}



\section{5a}

\subsection{Derivation of schemes with truncation errors}
All the schemes will be derived from Taylor series expansions, and the truncation error will be related to the remainder in the Taylor-series expansions. This remainder is the error we get when truncating the series by leaving out the remainder.\\

\subsection{Forward Euler}
For the time derivative, we expand $u(x, t + \Delta t)$ around $t$

\begin{subequations}
	\begin{align}
		u(x, t+ \Delta t)  = u(x,t) +  u_t(x,t) \Delta t + \mathcal{O}(\Delta t^2)\\
		\rightarrow u_t(x,t) = \frac{u(x, t+ \Delta t) - u(x,t)}{\Delta t} + \mathcal{O}(\Delta t)\label{eq:FeTime}
	\end{align}
\end{subequations}

The space derivative, which is a 2nd derivative, we derive by combining two Taylor series'

\begin{subequations}
	\begin{align}
		u(x + \Delta x,t) = u(x,t) + u_x(x,t)\Delta x + \frac{u_{xx}(x,t) \Delta x^2}{2} + \frac{u_{xxx}(x,t) \Delta x^3}{6} + \mathcal{O}(\Delta x^4)\label{eq:feSpace1}\\
		u(x - \Delta x,t) = u(x,t) - u_x(x,t)\Delta x + \frac{u_{xx}(x,t) \Delta x^2}{2} - \frac{u_{xxx}(x,t) \Delta x^3}{6} + \mathcal{O}(\Delta x^4)\label{eq:feSpace2}
	\end{align}
\end{subequations}

Now we add (\ref{eq:feSpace1}) and (\ref{eq:feSpace2}) and solve for $u_{xx}(x,t)$

\begin{subequations}
	\begin{align}
		\begin{split}
			\Big(u(x + \Delta x,t) + u(x - \Delta x,t) \Big) &= \Big(u(x,t) + u(x,t) \Big)\\ 
			&+ \Big(u_x(x,t)\Delta x + (- u_x(x,t)\Delta x) \Big)\\ 
			&+ \Big(\frac{u_{xx}(x,t) \Delta x^2}{2} +  \frac{u_{xx}(x,t) \Delta x^2}{2}\Big)\\ 
			&+ \Big(\frac{u_{xxx}(x,t) \Delta x^3}{6}  + (- \frac{u_{xxx}(x,t) \Delta x^3}{6}) \Big)\\ 
			&+ \Big(\mathcal{O}(\Delta x^4) + \mathcal{O}(\Delta x^4) \Big)
		\end{split}\\
		&= 2u(x,t) + u_{xx}(x,t) \Delta x^2 + \mathcal{O}(\Delta x^4)\\
		\rightarrow u_{xx}(x,t) &= \frac{u(x - \Delta x, t) - 2u(x,t) + u(x+ \Delta x, t)}{\Delta x^2} + \mathcal{O}(\Delta x^2)\label{eq:feSpace3}
	\end{align}
\end{subequations}

Combining (\ref{eq:FeTime}) and (\ref{eq:feSpace3}) we get the Forward Euler scheme

\begin{subequations}
	\begin{align}
		u_t(x,t) &= u_{xx}(x,t)\\
		\frac{u(x, t+ \Delta t) - u(x,t)}{\Delta t} + \mathcal{O}(\Delta t) &= 
		\frac{u(x - \Delta x, t) - 2u(x,t) + u(x+ \Delta x, t)}{\Delta x^2} + \mathcal{O}(\Delta x^2)\label{eq:fe}
	\end{align}
\end{subequations}

From (\ref{eq:fe}) we see that the scheme has a truncation error that goes like $\mathcal{O} (\Delta t)$ in time and $\mathcal{O}(\Delta x^2)$ in space.\\

We will analyze the stability of the Forward Euler scheme (\ref{eq:fe}) by applying Neuman stability analysis. From the analytical solution of the problem, we know that the particular solutions are on the form $u = e^{-(k \pi)^2 t}e^{i k \pi x}$, where $k$ is an integer greater than one. We observe that the solutions are stable in $t$, meaning that the solutions do not blow up as $t$ increaes. Based on the analytical particular solution, we make the numerical ansatz 

\begin{equation}\label{eq:neumanAnsatz}
	u = a_k^n e^{i k \pi x_j}
\end{equation}

For the numerical ansatz (\ref{eq:neumanAnsatz}) to reproduce the characteristics of the analytical particular solution, with stability in $t$, we observe that $|a_k^n| < 1$ is necessary. We now plug in the ansatz (\ref{eq:neumanAnsatz}) into the (\ref{eq:fe}) and derive an equation for $|a_k^n|$:

\begin{subequations}\label{eq:naumanFe0}
	\begin{align}
		\frac{u(x, t+ \Delta t) - u(x,t)}{\Delta t}  &= 
		\frac{u(x - \Delta x, t) - 2u(x,t) + u(x+ \Delta x, t)}{\Delta x^2} \\
		\frac{a_k^{n+1} e^{i k \pi (j+1) \Delta x} - a_k^n e^{i k \pi j \Delta x}}{\Delta t}  &= 
		\frac{a_k^{n} e^{i k \pi (j-1) \Delta x} - 2a_k^{n} e^{i k \pi j \Delta x} + a_k^{n} e^{i k \pi (j+1) \Delta x}}{\Delta x^2} \\
		a_k^{n} e^{i k \pi j \Delta x}\; \frac{a_k  -1}{\Delta t}  &= 
		a_k^{n} e^{i k \pi j \Delta x}\; \frac{ e^{-i k \pi \Delta x} - 2  +  e^{i k \pi  \Delta x}}{\Delta x^2} \\
		 \frac{a_k  -1}{\Delta t}  &= 
		 \frac{ e^{-i k \pi \Delta x} - 2  +  e^{i k \pi  \Delta x}}{\Delta x^2} \\
		 a_k &= 1 + \frac{\Delta t}{\Delta x^2} (e^{-i k \pi \Delta x} - 2  +  e^{i k \pi  \Delta x})\\
		 &= 1+ \frac{\Delta t}{\Delta x^2} \Big(2 \cos(k\pi\Delta x) - 2\Big)\\
		 &= 1+ 2\frac{\Delta t}{\Delta x^2} \Big( \cos(k\pi\Delta x) - 1\Big)\\
		 &= 1+ 2\frac{\Delta t}{\Delta x^2} \Big(- 2 \sin^2(\frac{k\pi\Delta x}{2}) \Big)\\
		 &=1 - 4\frac{\Delta t}{\Delta x^2} \sin^2(\frac{k\pi\Delta x}{2}) \\
		 |a_k| &= |1 - 4\frac{\Delta t}{\Delta x^2} \sin^2(\frac{k\pi\Delta x}{2})|\label{eq:neumanFe1}
	\end{align}
\end{subequations}

From (\ref{eq:neumanFe1}) we get
 
\begin{subequations}
	\begin{align}
		 &|a_k| < 1\; \text{if}\; ||1 - 4\frac{\Delta t}{\Delta x^2} \sin^2(\frac{k\pi\Delta x}{2})|| < 1 \\
		 &\rightarrow |1 - 4 \frac{\Delta t}{\Delta x^2}| < 1 \rightarrow |a_k| < 1\;\text{(Since $\sin^2(k \pi \Delta x/2)_{max}  = 1$)}\\
		 &\rightarrow 1 - 4 \frac{\Delta t}{\Delta x^2} > -1\\ 
		 &\rightarrow \frac{\Delta t}{\Delta x^2} < \frac{1}{2}\label{eq:neumanFe2}
	\end{align}
\end{subequations}

(\ref{eq:neumanFe2}) gives that the Forward Euler scheme is conditionally stable, and the condition that ensures stability.


\subsection{Backward Euler}
Here we will do the same as we did for Forward Euler above: Derive the scheme, including truncation errors, and analyze stability.\\

The only change compared to Forward Euler, is the time discretization, which now becomes

\begin{subequations}
	\begin{align}
	u(x, t- \Delta t)  = u(x,t) +  u_t(x,t) \Delta t - \mathcal{O}(\Delta t^2)\\
	\rightarrow u_t(x,t) = \frac{u(x, t) - u(x,t - \Delta t)}{\Delta t} + \mathcal{O}(\Delta t)\label{eq:beTime}
	\end{align}
\end{subequations}

The space discretization is the same as for Forward Euler, (\ref{eq:feSpace3}). Combining the space discretization (\ref{eq:feSpace3}) and (\ref{eq:beTime}) gives

\begin{subequations}
	\begin{align}
		\frac{u(x, t) - u(x,t - \Delta t)}{\Delta t} + \mathcal{O}(\Delta t) = \frac{u(x - \Delta x, t) - 2u(x,t) + u(x+ \Delta x, t)}{\Delta x^2} + \mathcal{O}(\Delta x^2)\label{eq:be1}
	\end{align}
\end{subequations}

We note that the truncation errors have the same asymptoptic behavior as for the Forward Euler scheme. \\

To analyze the stability of the Backward Euler scheme, we apply the same method as we did for Forward Euler and insert the ansatz (\ref{eq:neumanAnsatz}) into (\ref{eq:be1}) to get

\begin{subequations}
	\begin{align}
		\frac{u(x, t) - u(x,t - \Delta t)}{\Delta t}  &= \frac{u(x - \Delta x, t) - 2u(x,t) + u(x+ \Delta x, t)}{\Delta x^2} \\
		\frac{a_k^n e^{i k \pi j \Delta x} - a_k^{n-1} e^{i k \pi j \Delta x}}{\Delta t}  &= \frac{a_k^n e^{i k \pi (j -1)\Delta x} - 2a_k^n e^{i k \pi j \Delta x} + a_k^n e^{i k \pi (1 + j)\Delta x}}{\Delta x^2} \\
		a_k^n e^{i k \pi j \Delta x}\; \frac{1 - a_k^{-1}}{\Delta t}&=
		a_k^n e^{i k \pi j \Delta x}\; \frac{e^{-i k \pi \Delta x} - 2 + e^{i k \pi \Delta x}}{\Delta x^2}\\
		\frac{1 - a_k^{-1}}{\Delta t}&= \frac{e^{-i k \pi \Delta x} - 2 + e^{i k \pi \Delta x}}{\Delta x^2}\\
		a_k^{-1} &= 1 - \frac{\Delta t}{\Delta x^2} (e^{-i k \pi \Delta x} - 2 + e^{i k \pi \Delta x})\\
		a_k &= \frac{1}{1 - \frac{\Delta t}{\Delta x^2} (e^{-i k \pi \Delta x} - 2 + e^{i k \pi \Delta x})}\\
		&\stackrel{(\ref{eq:naumanFe0})}{=}  \frac{1}{1 + 4 \frac{\Delta t}{\Delta x^2} \sin^2(\frac{k\pi\Delta x}{2})}\\
		|a_k| &=  \left|\frac{1}{1 + 4 \frac{\Delta t}{\Delta x^2} \sin^2(\frac{k\pi\Delta x}{2})}\right| < 1.\label{eq:neumanBe}\\
	\end{align}
\end{subequations}

From (\ref{eq:neumanBe}) we see that, in contrast to the Forward Euler scheme, the Backward Euler scheme is unconditionally stable.\\

(\ref{eq:be1}) reveals another difference between the Backward Euler scheme and the Forward Euler scheme: (\ref{eq:be1}) is implicit in $u(x,t)$, meaning that we cannot solve (\ref{eq:be1}) directly for $u(x,t)$, as we did in the Forward Euler scheme. However, we can find $u(x,t)$ from (\ref{eq:be1}) by recognizing that (\ref{eq:be1}) can be rewritten as a linear system:

\begin{subequations}
	\begin{align}
		\frac{u(x, t) - u(x,t - \Delta t)}{\Delta t}  &= \frac{u(x - \Delta x, t) - 2u(x,t) + u(x+ \Delta x, t)}{\Delta x^2} \\
		\frac{u_i^n - u_i^{n-1}}{\Delta t}  &= \frac{u_{i-1}^n - 2u_i^n + u_{i+1}^n}{\Delta x^2} \\
		\frac{\Delta x^2}{\Delta t}(u_i^n - u_i^{n-1})  &=  u_{i-1}^n - 2u_i^n + u_{i+1}^n\\
		- \Big(u_{i-1}^n - (2 + \frac{\Delta x^2}{\Delta t}) u_i^n + u_{i+1}^n\Big)   &=  \frac{\Delta x^2}{\Delta t}u_i^{n-1}\\
		 \Big(-u_{i-1}^n + (2 + \frac{\Delta x^2}{\Delta t}) u_i^n - u_{i+1}^n\Big)   &=  \frac{\Delta x^2}{\Delta t}u_i^{n-1}\\
		\underbrace{\begin{bmatrix} 2 + \frac{\Delta x^2}{\Delta t} & -1 & \cdots & 0 \\ -1 & 2 + \frac{\Delta x^2}{\Delta t} & -1 & \vdots \\
			\vdots & &  \ddots & \vdots \\ 
			0 & \cdots & -1 & 2 + \frac{\Delta x^2}{\Delta t} \end{bmatrix}}_{\mathbf{A}} 
		\underbrace{\begin{bmatrix} u_1^n\\ u_2^n \\ \vdots\\ u_N^n \end{bmatrix}}_{\mathbf{U}} &= 
		\underbrace{\frac{\Delta x^2}{\Delta t} \begin{bmatrix} u_0^{n-1}\\ u_1^{n-1} \\ \vdots \\ u_N^{n-1}\end{bmatrix}}_{\mathbf{\tilde{b}}}\label{eq:beLinSys}
	\end{align}
\end{subequations}

We see from (\ref{eq:beLinSys}) that solving Backward Euler corresponds to solving a linear system $A U = \tilde{b}$, where $A$ is a trdiagonal matrix. 
	
\subsection{Crank-Nicolson}
Here we Taylor expand $u(x+\Delta x, t+\Delta t)$ and $u(x-\Delta x, t+\Delta t)$ around $t'=t+\Delta t/2$ to get

\begin{subequations}
	\begin{align}
		\begin{split}
			u(x+\Delta x, t+\Delta t)&=u(x,t')+\frac{\partial u(x,t')}{\partial x}\Delta x+\frac{\partial u(x,t')}{\partial t} \frac{\Delta t}{2} +\frac{\partial^2 u(x,t')}{2\partial x^2}\Delta x^2 +\frac{\partial^3 u(x,t')}{6\partial x^3}\Delta x^3\\
			&+\frac{\partial^2 u(x,t')}{2\partial t^2}\frac{\Delta t^2}{4} 
			+\frac{\partial^2 u(x,t')}{\partial x\partial t}\frac{\Delta t}{2} \Delta x+ \mathcal{O}(\Delta t^3) + \mathcal{O}(\Delta x^4)
		\end{split}\label{eq:cn1}\\
		\begin{split}
			u(x-\Delta x,t+ \Delta t)&=u(x,t')
			-\frac{\partial u(x,t')}{\partial x}\Delta x 
			+ \frac{\partial u(x,t')}{\partial t} \frac{\Delta t}{2} + 
			\frac{\partial^2 u(x,t')}{2\partial x^2}\Delta x^2 
			-\frac{\partial^3 u(x,t')}{6\partial x^3}\Delta x^3\\
			&+\frac{\partial^2 u(x,t')}{2\partial t^2}\frac{\Delta t^2}{4} 
			- \frac{\partial^2 u(x,t')}{\partial x\partial t}\frac{\Delta t}{2} \Delta x
			+ \mathcal{O}(\Delta t^3) + \mathcal{O}(\Delta x^4)
		\end{split}\label{eq:cn2}\\
		\begin{split}
			u(x+\Delta x,t)&=u(x,t')
			+\frac{\partial u(x,t')}{\partial x}\Delta x
			-\frac{\partial u(x,t')}{\partial t} \frac{\Delta t}{2} +\frac{\partial^2 u(x,t')}{2\partial x^2}\Delta x^2
			 +\frac{\partial^3 u(x,t')}{6\partial x^3}\Delta x^3\\
			&+\frac{\partial^2 u(x,t')}{2\partial t^2}\frac{\Delta t^2}{4} 
			- \frac{\partial^2 u(x,t')}{\partial x\partial t}\frac{\Delta t}{2} \Delta x+ \mathcal{O}(\Delta t^3) + \mathcal{O}(\Delta x^4)
		\end{split}\label{eq:cn3}\\
		\begin{split}
		u(x-\Delta x,t)&=u(x,t')-\frac{\partial u(x,t')}{\partial x}\Delta x-\frac{\partial u(x,t')}{\partial t} \frac{\Delta t}{2} 
		+ \frac{\partial^2 u(x,t')}{2\partial x^2}\Delta x^2
		-\frac{\partial^3 u(x,t')}{6\partial x^3}\Delta x^3\\
		&
		+\frac{\partial^2 u(x,t')}{2\partial t^2}\frac{\Delta t^2}{4}
		+\frac{\partial^2 u(x,t')}{\partial x\partial t}\frac{\Delta t}{2} \Delta x
		+ \mathcal{O}(\Delta t^3) + \mathcal{O}(\Delta x^4)
		\end{split}\label{eq:cn4}\\
		u(x,t+\Delta t)&=u(x,t')+\frac{\partial u(x,t')}{\partial t}\frac{\Delta_t}{2} +\frac{\partial ^2 u(x,t')}{2\partial t^2}\Delta t^2 + \mathcal{O}(\Delta t^3)\label{eq:cn5}\\
		u(x,t)&=u(x,t')-\frac{\partial u(x,t')}{\partial t}\frac{\Delta t}{2}+\frac{\partial ^2 u(x,t')}{2\partial t^2}\Delta t^2 + \mathcal{O}(\Delta t^3)\label{eq:cn6}
	\end{align}
\end{subequations}

The above formulae are taken from Hjorth-Jensen's  \href{https://github.com/CompPhysics/ComputationalPhysics/tree/master/doc/pub/pde}{slides} \cite{MHJ2}.\\

Combining (\ref{eq:cn5}) and (\ref{eq:cn6}) gives the time derivative

\begin{subequations}
	\begin{align}
		\begin{split}
			\Big(u(x,t+\Delta t) - u(x,t)\Big) &= \Big(u(x,t') -  u(x,t')\Big) + \Big(\frac{\partial u(x,t')}{\partial t}\frac{\Delta_t}{2} - (-\frac{\partial u(x,t')}{\partial t}\frac{\Delta t}{2}) \Big)\\ 
			&+ \Big(\frac{\partial ^2 u(x,t')}{2\partial t^2}\Delta t^2 - \frac{\partial ^2 u(x,t')}{2\partial t^2}\Delta t^2 \Big) +\Big(\mathcal{O}(\Delta t^3) -  \mathcal{O}(\Delta t^3)\Big)
		\end{split}\\
		u(x,t+\Delta t) - u(x,t)& = \frac{\partial u(x,t')}{\partial t}\Delta t + \mathcal{O}(\Delta t^3)\\
		\frac{\partial u(x,t')}{\partial t}&= \frac{u(x,t+\Delta t) - u(x,t)}{\Delta t} + \mathcal{O}(\Delta t^2)\label{eq:cnTime}
	\end{align}
\end{subequations}

Now for the spacial derivative. First we solve (\ref{eq:cn1}) and (\ref{eq:cn2}) for $u_{xx}(x, t')$

\begin{subequations}
	\begin{align}
		\begin{split}
		u(x+\Delta x, t+\Delta t) + u(x-\Delta x,t + \Delta t) &= 
		\Big(u(x,t') + u(x,t') \Big) + \Big(\frac{\partial u(x,t')}{\partial x}\Delta x-\frac{\partial u(x,t')}{\partial x}\Delta x\Big)\\ 
		&+ \Big(\frac{\partial u(x,t')}{\partial t} \frac{\Delta t}{2} +\frac{\partial u(x,t')}{\partial t} \frac{\Delta t}{2} \Big) + \Big(\frac{\partial^2 u(x,t')}{2\partial x^2}\Delta x^2+\frac{\partial^2 u(x,t')}{2\partial x^2}\Delta x^2\Big)\\
		&+ \Big(\frac{\partial u(x,t')}{\partial t} \frac{\Delta t}{2} +\frac{\partial u(x,t')}{\partial t} \frac{\Delta t}{2} \Big) + \Big(\frac{\partial^3 u(x,t')}{6\partial x^3}\Delta x^3-\frac{\partial^3 u(x,t')}{6\partial x^3}\Delta x^3\Big)\\
		& + \Big(\frac{\partial^2 u(x,t')}{2\partial t^2}\frac{\Delta t^2}{4} +\frac{\partial^2 u(x,t')}{2\partial t^2}\frac{\Delta t^2}{4} \Big) + \Big(\frac{\partial^2 u(x,t')}{\partial x\partial t}\frac{\Delta t}{2} \Delta x - \frac{\partial^2 u(x,t')}{\partial x\partial t}\frac{\Delta t}{2} \Delta x\Big)\\
		& + \Big(\mathcal{O}(\Delta t^3) + \mathcal{O}(\Delta t^3) \Big) + \mathcal{O}(\Delta x^4)
		\end{split}\\
		\begin{split}
			&=2u(x,t') + \frac{\partial u(x,t')}{\partial t} \Delta t
			+ \frac{\partial^2 u(x,t')}{\partial x^2}\Delta x^2
			 + \frac{\partial^2 u(x,t')}{2\partial t^2}\frac{\Delta t^2}{2}  
			 + \mathcal{O}(\Delta t^3) + \mathcal{O}(\Delta x^4)
		\end{split}\\
		\begin{split}
			&=2u(x,t') + \frac{\partial u(x,t')}{\partial t} \Delta t
			+ \frac{\partial^2 u(x,t')}{\partial x^2}\Delta x^2			
			+ \mathcal{O}(\Delta t^2) + \mathcal{O}(\Delta x^4)
		\end{split}\\	
		\begin{split}
		\frac{\partial^2 u(x,t')}{\partial x^2}\Delta x^2
		& = 
		u(x+\Delta x, t+\Delta t) + u(x-\Delta x,t + \Delta t) 
		- 2u(x,t') 
			\\ 
		&+\frac{\partial u(x,t')}{\partial t} \Delta t
		+  \mathcal{O}(\Delta t^2) + \mathcal{O}(\Delta x^4)
		\end{split}\\
		\begin{split}
			\frac{\partial^2 u(x,t')}{\partial x^2}
			&=\frac{u(x-\Delta x,t+ \Delta t)  - 2u(x,t') +  	u(x+\Delta x, t+\Delta t)}{\Delta x^2}\\
			& +\frac{\partial u(x,t')}{\partial t} \frac{\Delta t}{\Delta x^2} +\mathcal{O}(\Delta t^2) + \mathcal{O}(\Delta x^2)\label{eq:cnX2}
		\end{split}		
	\end{align}
\end{subequations}

Doing the same as in (\ref{eq:cnX2}) for (\ref{eq:cn3}) and (\ref{eq:cn4}) we obtain

\begin{subequations}\label{eq:cnX3}
	\begin{align}
		\begin{split}
			\frac{\partial^2 u(x,t')}{\partial x^2} &=\frac{u(x-\Delta x,t) - 2u(x,t') + u(x+\Delta x, t)}{\Delta x^2}\\
			&- \frac{\partial u(x,t')}{\partial t} \frac{\Delta t}{\Delta x^2} 
			+  \mathcal{O}(\Delta t^2) + \mathcal{O}(\Delta x^2)
		\end{split}
	\end{align}
\end{subequations}

Now we take the mean of (\ref{eq:cnX2}) and (\ref{eq:cnX3}) 

\begin{subequations}
	\begin{align}
		\begin{split}
			u_{xx}(x, t^{'}) &=\frac{1}{2} \Big(\frac{u(x-\Delta x,t+ \Delta t)  - 2u(x,t') +  	u(x+\Delta x, t+\Delta t)}{\Delta x^2} +\frac{\partial u(x,t')}{\partial t} \frac{\Delta t}{\Delta x^2} + \mathcal{O}(\Delta t^2) + \mathcal{O}(\Delta x^2) \\
			&+  \frac{u(x-\Delta x,t) - 2u(x,t') + u(x+\Delta x, t)}{\Delta x^2} -\frac{\partial u(x,t')}{\partial t} \frac{\Delta t}{\Delta x^2} + \mathcal{O}(\Delta t^2)  + \mathcal{O}(\Delta x^2)\Big)\\
			&= \frac{1}{2} \Big(\frac{u(x-\Delta x,t+ \Delta t)  - 2u(x,t') +  	u(x+\Delta x, t+\Delta t)}{\Delta x^2} \\
			& +  \frac{u(x-\Delta x,t) - 2u(x,t') + u(x+\Delta x, t)}{\Delta x^2} \Big) + \mathcal{O}(\Delta t^2) + \mathcal{O}(\Delta x^2)\\
			&\stackrel{u(x,t^{'}) = \frac{u(x,t) + u(x,t+\Delta t)}{2}}{=}
			\frac{1}{2} \Big(\frac{u(x-\Delta x,t+ \Delta t)  - u(x,t) + u(x,t+\Delta t) +  	u(x+\Delta x, t+\Delta t)}{\Delta x^2} \\
			& +  \frac{u(x-\Delta x,t) - u(x,t) + u(x,t+\Delta t) + u(x+\Delta x, t)}{\Delta x^2} \Big) + \mathcal{O}(\Delta t^2) + \mathcal{O}(\Delta x^2)\\
			&= \frac{1}{2} \Big(\frac{u(x-\Delta x,t+ \Delta t) - 2u(x,t+\Delta t) +u(x + \Delta x,t+\Delta t)}{\Delta x^2} + \frac{u(x-\Delta x,t) - 2u(x,t) +u(x+ \Delta x,t)}{\Delta x^2} \Big)\\
			& + \mathcal{O}(\Delta t^2)  + \mathcal{O}(\Delta x^2)\label{eq:cnX4}
			\end{split}
	\end{align}
\end{subequations}

Now combining (\ref{eq:cnTime}) and (\ref{eq:cnX4}) we get the Crank-Nicolson scheme

\begin{subequations}\label{eq:cn1d}
	\begin{align}
		\begin{split}
			\frac{u(x,t+\Delta t) - u(x,t)}{\Delta t} + \mathcal{O}(\Delta t^2)
			&= \frac{1}{2} \Big(\frac{u(x-\Delta x,t+ \Delta t)
			- 2u(x,t+\Delta t) +u(x+\Delta x,t+\Delta t)}{\Delta x^2} \\
			&+ \frac{u(x-\Delta x,t) - 2u(x,t) +u(x+ \Delta x,t)}{\Delta x^2} \Big) + \mathcal{O}(\Delta x^2),
		\end{split}
	\end{align}
\end{subequations}

where we have put both $\mathcal{O}(\Delta t^2)$ into a common term.\\

(\ref{eq:cn1d}) shows that the Crank-Nicolson scheme is 2nd order in both time and space, implying better convergence properties for the Crank-Nicolson scheme compared to the Backward Euler scheme and the Forward Euler scheme.\\

We study the stability of the Crank-Nicolson scheme using the same method as for the previous schemes. Insertion of the ansatz (\ref{eq:neumanAnsatz}) into the Crank-Nicolson scheme (\ref{eq:cn1d}) and solving for $|a_k|$ gives:

\begin{subequations}
	\begin{align}
		\begin{split}
			\frac{u(x,t+\Delta t) - u(x,t)}{\Delta t} 
			&= \frac{1}{2} \Big(\frac{u(x-\Delta x,t+ \Delta t)
			- 2u(x,t+\Delta t) +u(x+\Delta x,t+\Delta t)}{\Delta x^2} \\
			&+ \frac{u(x-\Delta x,t) - 2u(x,t) +u(x+ \Delta x,t)}{\Delta x^2} \Big) \\
			a_k^n e^{i k \pi j \Delta x}\; \frac{ a_k  - 1}{\Delta t} &= \frac{a_k^n e^{i k \pi j \Delta x}}{2}\; \Big(\frac{a_k e^{-i k \pi \Delta x} - 2a_k + a_k e^{i k \pi \Delta x}}{\Delta x^2} + \frac{e^{-i k \pi \Delta x} - 2 +  e^{i k \pi \Delta x}}{\Delta x^2} \Big)\\
			\frac{ a_k  - 1}{\Delta t}&=\frac{1}{2}\Big(\frac{a_k e^{-i k \pi \Delta x} - 2a_k + a_k e^{i k \pi \Delta x}}{\Delta x^2} + \frac{e^{-i k \pi \Delta x} - 2 +  e^{i k \pi \Delta x}}{\Delta x^2} \Big)\\
			&= \frac{1+a_k}{2\Delta x^2} (e^{-i k \pi \Delta x} - 2 + e^{i k \pi \Delta x})\\
			&\stackrel{(\ref{eq:naumanFe0})}{=} -4\frac{1+a_k}{2\Delta x^2} \sin^2(\frac{k\pi \Delta x}{2})\\
			a_k -1 &= (1 + a_k) (-\frac{2 \Delta t}{\Delta x^2})\sin^2(\frac{k\pi \Delta x}{2})\\
			\Big(1 +\frac{2 \Delta t}{\Delta x^2}\sin^2(\frac{k\pi \Delta x}{2})\Big)a_k &= 1 - \frac{2 \Delta t}{\Delta x^2}\sin^2(\frac{k\pi \Delta x}{2})
			\end{split}\\
			a_k &= \frac{1 - \frac{2 \Delta t}{\Delta x^2}\sin^2(\frac{k\pi \Delta x}{2})}{1 +\frac{2 \Delta t}{\Delta x^2}\sin^2(\frac{k\pi \Delta x}{2})} < 1\label{eq:neumanCn1d}
	\end{align}
\end{subequations}

(\ref{eq:neumanCn1d}) shows that the Crank-Nicolson scheme is unconditionally stable.\\

Based on the analyzis of the different schemes, we expect the Crank-Nicolson scheme to be the best scheme with respect to convergence and stability. 

\subsection{$\theta$-rule}
All the schemes derived above can be derived from a more general scheme, called the $\theta$-rule scheme. To see this, first notice that the Crank-Nicolson scheme (\ref{eq:cn1d}) is the average of the Forward Euler scheme (\ref{eq:fe}) and the Backward Euler scheme (\ref{eq:be1}). If we think of the $\theta$-rule as a weighted average with weights $\theta$ and $1-\theta$ so that $u_{t} = \theta u_{xx,BE} + (1- \theta) u_{xx,FE}$, we see that the Crank-Nicolson shceme is just a special case of the $\theta$-scheme with equal weights, $\theta = 1/2$. Based on this reasoning, we get the $\theta$-rule

\begin{subequations}
	\begin{align}
		\begin{split}
			(1-\theta) 	\frac{u(x,t+\Delta t) - u(x,t)}{\Delta t} + \theta 	\frac{u(x,t+\Delta t) - u(x,t)}{\Delta t}&=  \theta \frac{u(x-\Delta x,t+ \Delta t)
				- 2u(x,t+\Delta t) +u(x+\Delta x,t+\Delta t)}{\Delta x^2} \\
			&+ (1 - \theta) \frac{u(x-\Delta x,t) - 2u(x,t) +u(x+ \Delta x,t)}{\Delta x^2} 
		\end{split}\\
		\begin{split}
			\frac{u(x,t+\Delta t) - u(x,t)}{\Delta t}&= 
			  \theta \frac{u(x-\Delta x,t+ \Delta t)
				- 2u(x,t+\Delta t) +u(x+\Delta x,t+\Delta t)}{\Delta x^2} \\
			&+ (1 - \theta) \frac{u(x-\Delta x,t) - 2u(x,t) +u(x+ \Delta x,t)}{\Delta x^2} 
			\end{split}\label{eq:thetaRule}
	\end{align}
\end{subequations}

From the $\theta$-scheme (\ref{eq:thetaRule}) we see that $\theta = 0,\;1/2,\;1$ corresponds to the Forward Euler, the Crank-Nicolson and the Backward Euler scheme respectively.\\

When implementing the 1D-schemes, we will use the $\theta$-scheme. Using the $\theta$-scheme, we need only write one scheme instead of three.

\subsection{Implementation of 1D problem}
As mentioned in the previous paragraph, we implement the 1D schemes using the $\theta$-scheme. We will now rewrite (\ref{eq:thetaRule}) to a linear system, and then we will apply the Thomas algorithm to this system.\\

To ease the notation, we introduce 

\begin{equation}\label{eq:alpha}
	\alpha = \frac{\Delta t}{\Delta x^2}.
\end{equation}

Insertion of $\alpha$ (\ref{eq:alpha}) into the $\theta$-scheme (\ref{eq:thetaRule}) and introdusing the discretization $u(x + \Delta x, t - \Delta t) = u_{i+1}^{n-1}$ gives

\begin{subequations}
	\begin{align}
		\begin{split}
				&\frac{u(x,t+\Delta t) - u(x,t)}{\Delta t}= 
		\theta \frac{u(x-\Delta x,t+ \Delta t)
			- 2u(x,t+\Delta t) +u(x+\Delta x,t+\Delta t)}{\Delta x^2} \\
		&+ (1 - \theta) \frac{u(x-\Delta x,t) - 2u(x,t) +u(x+ \Delta x,t)}{\Delta x^2} 
		\end{split}\\
		\begin{split}
			&u(x,t+\Delta t) - u(x,t)= 
			\alpha \theta \Big(u(x-\Delta x,t+ \Delta t)
				- 2u(x,t+\Delta t) +u(x+\Delta x,t+\Delta t)\Big) \\
			&+ \alpha (1 - \theta) \Big(u(x-\Delta x,t) - 2u(x,t) +u(x+ \Delta x,t)\Big) 
		\end{split}\\
			&u_i^{n+1} - u_i^{n}= 
			\alpha \theta \Big(u_{i-1}^{n+1}
			- 2u_i^{n+1} +u_{i+1}^{n+1}\Big) + \alpha (1 - \theta) \Big(u_{i-1}^n - 2u_i^n +u_{i+1}^n\Big) \\
		&u_i^{n} - u_i^{n-1}= 
		\alpha \theta \Big(u_{i-1}^{n}
		- 2u_i^{n} +u_{i+1}^{n}\Big) + \alpha (1 - \theta) \Big(u_{i-1}^{n-1} - 2u_i^{n-1} +u_{i+1}^{n-1}\Big) \\
			&u_i^{n} -\alpha \theta \Big(u_{i-1}^{n}
			- 2u_i^{n} +u_{i+1}^{n}\Big) = u_i^{n-1} 
			 + \alpha (1 - \theta) \Big(u_{i-1}^{n-1} - 2u_i^{n-1} +u_{i+1}^{n-1}\Big)\\
			&-\alpha \theta u_{i-1}^{n} +
			(2 \alpha \theta + 1) u_i^{n} - \alpha \theta u_{i+1}^{n} = 
			 \alpha (1 - \theta) u_{i-1}^{n-1} +\Big(1 - 2 \alpha (1 - \theta) \Big)u_i^{n-1} +\alpha (1 - \theta)u_{i+1}^{n-1}\\
			 &-2\alpha \theta u_{i-1}^{n} +
			 2(2 \alpha \theta + 1) u_i^{n} - 2\alpha \theta u_{i+1}^{n} = 
			 2\alpha (1 - \theta) u_{i-1}^{n-1} +2\Big(1 - 2 \alpha (1 - \theta) \Big)u_i^{n-1} +2\alpha (1 - \theta)u_{i+1}^{n-1}\label{eq:thetaAlgo1}
	\end{align}
\end{subequations}

Now we rewrite (\ref{eq:thetaAlgo1}) as a matrix-vector equation $A_1 U^n = A_2 U^{n-1}$:

\begin{subequations}
	\begin{align}
		\begin{split}
		\underbrace{\begin{bmatrix} 2(2 \alpha \theta + 1) & - 2\alpha \theta & \cdots & 0 \\ - 2\alpha \theta & 2(2 \alpha \theta + 1) & - 2\alpha \theta & \vdots \\
			\vdots & &  \ddots & - 2\alpha \theta \\ 
			0 & \cdots & - 2\alpha \theta &2(2 \alpha \theta + 1) \end{bmatrix}}_{\mathbf{A_1}} 
		\underbrace{\begin{bmatrix} u_1^n\\ u_2^n \\ \vdots\\ u_N^n \end{bmatrix}}_{\mathbf{U^{n}}}\\ = 
		\underbrace{\begin{bmatrix} 2( 1 - 2 \alpha (1-\theta)) &  2\alpha (1-\theta) & \cdots & 0 \\  2\alpha (1-\theta) & 2( 1 - 2 \alpha (1-\theta)) &  2\alpha (1-\theta) & \vdots \\
	\vdots & &  \ddots &  2\alpha (1-\theta) \\ 
	0 & \cdots &  2\alpha (1-\theta) &2( 1 - 2 \alpha (1-\theta)) \end{bmatrix}}_{\mathbf{A_2}} 
	\underbrace{\begin{bmatrix} u_1^{n-1}\\ u_2^{n-1} \\ \vdots\\ u_N^{n-1} \end{bmatrix}}_{\mathbf{U^{n-1}}}	
		\end{split}\label{eq:thetaLinsSys}
	\end{align}
\end{subequations}

In (\ref{eq:thetaLinsSys}), the right hand side $\mathbf{A_2 U^{n-1}}$ is known, since $\mathbf{U}^{n-1}$ is the previous periods solution. Hence (\ref{eq:thetaLinsSys}) is a linear system of the known type $A U = b$, which needs to be solved at each time step.\\

We note that with $\theta = 0$ in (\ref{eq:thetaLinsSys}), the system reduces to the explicit Forward Euler system, which can be obtained from (\ref{eq:fe}). $\theta  = 1$ gives the Backward Euler system (\ref{eq:beLinSys}). $\theta = 1/2$ in (\ref{eq:thetaLinsSys}) gives the system that can be obtained by rewrting the Crank-Nicolson scheme (\ref{eq:cn1d}) as a linear system.\\

In principle the above system can be solved by calculating the inverse of $A_1$ and solving $U^n = A_1^{-1} (A_2 U^{n-1})$. This is a costly operation and is avoided. Instead of calculating the inverse, one often solves these systems by a elimination method, e.g. Gaussian elimination or by LU. In this case, we note that we are dealing with a tridiagonal matrix. For tridiagonal systems, the Thomas algorithm gives an alterative way of solving the linear system which reduces the number of FLOPS considerably. In our case we are eaven more lucky. Our matrix is symmetric, and the elements are constant, so the standard Thomas algorithm can be further improved. We will now derive the algorithm that we implement.\\

We start with a tridiagonal symmetric linear system $Au =f$, with $A$ being $4 \times 4$. The following show the forward substitution steps for this sytem

\begin{subequations}
	\begin{align}
		\begin{bmatrix}
		     d_1 & e_1 & 0 & 0 & f_1\\
		      e_2 & d_2 & e_2 & 0 & f_2\\
			 0 & e_3 & d_3 & e_3 & f_3\\
	         0 & 0 & e_4 & d_4 & f_4
		\end{bmatrix}
		\rightarrow
		\begin{bmatrix}
		d_1 & e_1 & 0 & 0 & f_1\\
		0 & \tilde{d}_2 & e_2 & 0 & \tilde{f}_2\\
		0 & e_3 & d_3 & e_3 & f_3\\
		0 & 0 & e_4 & d_4 & f_4
		\end{bmatrix}
		\rightarrow
		\begin{bmatrix}
		d_1 & e_1 & 0 & 0 & f_1\\
		0 & \tilde{d}_2 & e_2 & 0 &\tilde{f}_2 \\
		0 & 0 & \tilde{d}_3 & e_3 & \tilde{f}_3\\
		0 & 0 & e_4 & d_4 & f_4
		\end{bmatrix}
		\rightarrow
		\begin{bmatrix}
		d_1 & e_1 & 0 & 0 & f_1\\
		0 & \tilde{d}_2 & e_2 & 0 &\tilde{f}_2 \\
		0 & 0 & \tilde{d}_3 & e_3 & \tilde{f}_3\\
		0 & 0 & e_4 & \tilde{d}_4 & \tilde{f}_4
		\end{bmatrix}
	\end{align}\label{eq:thomas1}
\end{subequations}


From (\ref{eq:thetaLinsSys}) we have that in our case $e_1=e_2 = ... = e_N= -2 \alpha$ and $d_1 = d_2 = ... = d_N = 2(2\alpha \theta +1)$. Calculating the first couple of $\tilde{d}$'s we quickly see that we get the following general expression for $\tilde{d}_i$

\begin{equation}\label{eq:thomasDtilde}
	\tilde{d}_i = 2(2\alpha \theta +1) + \frac{(2\alpha)^2}{\tilde{d}_{i-1}}\;\text{for } i>1.
\end{equation}

Doing the same for $\tilde{f}$ we get the general expression

\begin{equation}\label{eq:thomasFtilde}
	\tilde{f}_i = f_i + \frac{2 \alpha \theta}{\tilde{d}_{i-1}} \tilde{f}_{i-1}\;\text{for } i>1.
\end{equation}

Having $\tilde{d}$ and $\tilde{f}$ we are ready to do the back substitution step. We have

\begin{subequations}
	\begin{align}
		\begin{bmatrix}
		d_1 & e_1 & 0 & 0\\
		0 & \tilde{d}_2 & e_2 & 0  \\
		0 & 0 & \tilde{d}_3 & e_3 \\
		0 & 0 & e_4 & \tilde{d}_4 
		\end{bmatrix}
		\begin{bmatrix} u_1\\ u_2 \\ u_3\\ u_4  \end{bmatrix}
		= \begin{bmatrix} f_1\\\tilde{f}_2 \\ \tilde{f}_3\\ \tilde{f}_4  \end{bmatrix}
	\end{align}\label{eq:thomasBack1}
\end{subequations}

From (\ref{eq:thomasBack1}) we get

\begin{equation}\label{eq:thomasULast}
	u_4 = \frac{\tilde{f}_4}{\tilde{d}_4}
\end{equation}

and

\begin{equation}\label{eq:thomasU}
	u_i = \frac{\tilde{f}_i + 2\alpha \theta u_{i+1}}{\tilde{d}_i}\; \text{for } i > 1.
\end{equation}

The equations (\ref{eq:thomasDtilde}), (\ref{eq:thomasFtilde}), (\ref{eq:thomasULast}) and (\ref{eq:thomasU}) is what we need for our algorithm.

\begin{lstlisting}
for time in times:
	Calculate RHS f (= A_2 U^{n-1})
	
	// Forward substitution
	for row in rows (Start from 1st row):
		Calculate \tilde{d}
		Calculate \tilde{f}
	end row loop
	
	// Backward substitution
	For row in rows (Starting from last row)
		Calculate u
	end row loop

	// Update RHS for next time step
	U^{n-1} = U^{n}

end time loop
\end{lstlisting}

\subsection{2D schemes}
We will derive and apply a Forward Euler (explicit) scheme and a Backward Euler (implicit) scheme for the 2D case.

\subsubsection{2D explicit scheme}
Compared to the 1D case, we get an extra term for $u_{yy}$ in (\ref{eq:fe}) for the explicit scheme. $u_{yy}$ is approximated in exactly the same way as $u_{xx}$ (\ref{eq:feSpace3}), so we get

\begin{equation}
	 u_{yy}(x,y,t) = \frac{u(x,y - \Delta y, t) - 2u(x,y,t) + u(x,y+ \Delta y, t)}{\Delta y^2} + \mathcal{O}(\Delta y^2)\label{eq:uyyFe2d}
\end{equation}

Adding (\ref{eq:uyyFe2d}) to the 1D FE scheme (\ref{eq:fe}) gives the explicit 2D-scheme

\begin{subequations}
	\begin{align}
		\begin{split}
			\frac{u(x,y, t+ \Delta t) - u(x,y,t)}{\Delta t} + \mathcal{O}(\Delta t) &= 
			\frac{u(x - \Delta x, y,t) - 2u(x,y,t) + u(x+ \Delta x,y, t)}{\Delta x^2} \\
			&+  \frac{u(x,y - \Delta y, t) - 2u(x,y,t) + u(x,y+ \Delta y, t)}{\Delta y^2} \\
			&+ \mathcal{O}(\Delta x^2)  + \mathcal{O}(\Delta y^2)
		\end{split}\\
		\rightarrow u_{ij}^{n+1} &\stackrel{(\ref{eq:alpha})}{\approx} u_{ij}^{n} + \alpha (u_{i-1,j}^n + u_{i+1,j}^n + u_{i,j-1}^n + u_{i,j+1}^n - 4u_{ij}^n),\label{eq:Fe2d}
	\end{align}
\end{subequations}

where we in the last transition assumed $\Delta x = \Delta y$ such that $\alpha$ is given as before.\\

First we observe that the truncation errors go like $\mathcal{O}(\Delta t)$, $\mathcal{O}(\Delta x^2)$ and $\mathcal{O}(\Delta y^2)$.\\

We analyze the stability by inserting the ansatz

\begin{equation}\label{eq:ansatz2d}
	u = a_k^n e^{i k \pi \Delta x j} e^{i k \pi \Delta y l}
\end{equation}

into (\ref{eq:Fe2d}) and solve for $|a|$, as before\\

\begin{subequations}
	\begin{align}
		u_{ij}^{n+1} &= u_{ij}^{n} + \alpha (u_{i-1,j}^n + u_{i+1,j}^n + u_{i,j-1}^n + u_{i,j+1}^n - 4u_{ij}^n)\\
		a_k^n e^{i \pi k \Delta x j} e^{i \pi k \Delta j l} a&= 
		a_k^n e^{i \pi k \Delta x j} e^{i \pi k \Delta j l} 
		\Big(1 + \alpha(e^{-i k \pi \Delta x} + e^{i k \pi \Delta x})  - 2 + e^{-i k \pi \Delta y} + e^{i k \pi \Delta y})  - 2\Big)\\
		a&= 
		\Big(1 + \alpha(e^{-i k \pi \Delta x} + e^{i k \pi \Delta x})  - 2 + e^{-i k \pi \Delta y} + e^{i k \pi \Delta y})  - 2\Big)\\
		&\stackrel{(\ref{eq:naumanFe0})}{=}
		1 - 4 \alpha \Big(\sin^2 (k \pi \Delta x /2) + \sin^2(k \pi \Delta y/2)\Big)\\
		|a| &=
		\left|1 - 4 \alpha \Big(\sin^2 (k \pi \Delta x /2) + \sin^2(k \pi \Delta y/2)\Big)\right|\\
		|a| < 1 &\rightarrow 		\left|1 - 4 \alpha \Big(\sin^2 (k \pi \Delta x /2) + \sin^2(k \pi \Delta y/2)\Big)\right|  < 1\label{eq:2dFeStab1}\\
		&\left|1 - 4 \alpha \Big(\sin^2 (k \pi \Delta x /2) + \sin^2(k \pi \Delta y/2)\Big)\right| < \left|1 - 8 \alpha \right|\label{eq:2dFeStab2}
	\end{align}
\end{subequations}

Again assuming $\Delta x = \Delta y$, we have $\alpha = \Delta t/\Delta x^2 > 0$, so that (\ref{eq:2dFeStab1}) and (\ref{eq:2dFeStab2}) gives

\begin{subequations}
	\begin{align}
		1 - 8 \alpha < -1\\
		\alpha < \frac{1}{4}.\label{eq:2dFeStab}
	\end{align}
\end{subequations}

(\ref{eq:2dFeStab}) gives that the 2D Forward Euler scheme is conditionally stable. We note that we could probably have applied a simpler ansatz than (\ref{eq:ansatz2d}), since it in (\ref{eq:Fe2d}) was already assumed that $\Delta x = \Delta y$. \\

The new thing in (\ref{eq:Fe2d}), compared to the 1D case (\ref{eq:fe}), is that $u{ij}^{n+1}$ in the 2D case is a matrix and not a vector. The fact that we are dealing with a matrix instead of a vector, does not change anything with regards to the solution strategy. The equation (\ref{eq:Fe2d}) is still explicit, the only difference in implementation is that instead of one single loop, a double loop over two space dimensions is needed now.\\

The algorithm for solving 2D Forward Euler follows.

\begin{lstlisting}
Set IC

for time in times:
	Set BC
	
	for x in X:
		for y in Y:
			Calculate u_{ij}^{n+1} = f(u^{n})
		end y-loop
	end x-loop
	
	Set u^{n} = u^{n+1}	
	
end time-loop
\end{lstlisting}

\subsubsection{2D implicit scheme}
We start in the same way as we did for Forward Euler in 2D, and just add an extra term for $u_{yy}$ to the 1D Backward Euler scheme (\ref{eq:be1}). In addition we will assume $h = \Delta x = \Delta y$.

\begin{subequations}
	\begin{align}
		\begin{split}
			\frac{u(x,y, t) - u(x,y,t - \Delta t)}{\Delta t} + \mathcal{O}(\Delta t) &= \frac{u(x - \Delta x,y, t) 
				- 2u(x,y,t) + u(x+ \Delta x,y, t)}{\Delta x^2} \\
			&+ \frac{u(x,y - \Delta y, t) - 2u(x,y,t) + u(x,y+ \Delta y, t)}{\Delta y^2}\\
			&+ \mathcal{O}(\Delta x^2) + \mathcal{O}(\Delta y^2)
		\end{split}\\
		\begin{split}
		\frac{u_{i,j}^n - u_{i,j}^{n-1}}{\Delta t}  &\approx \frac{u_{i-1,j}^{n} 
			- 2u_{i,j}^n + u_{i+1,j}^n}{\Delta x^2} \\
		&+ \frac{u_{i,j-1}^n - 2u_{i,j}^n + u_{i,j+1}^n}{\Delta y^2}
		\end{split}\\
		\begin{split}
			\frac{u_{i,j}^n - u_{i,j}^{n-1}}{\Delta t} &\stackrel{h = \Delta x = \Delta y}{\approx} \frac{u_{i-1,j}^{n} 
			- 2u_{i,j}^n + u_{i+1,j}^n}{h^2} \\
			&+ \frac{u_{i,j-1}^n - 2u_{i,j}^n + u_{i,j+1}^n}{h^2}
		\end{split}\\				
		u_{i,j}^n - u_{i,j}^{n-1}  &\stackrel{(\ref{eq:alpha})}{\approx} \alpha (u_{i-1,j}^{n} 
			- 4u_{i,j}^n + u_{i+1,j}^n + u_{i,j-1}^n + u_{i,j+1}^n)\label{eq:2dBe1}\\
		u_{i,j}^n &= u_{i,j}^{n-1}  + \frac{1}{1+4\alpha}(u_{i-1,j}^{n} 
		+ u_{i+1,j}^n + u_{i,j-1}^n + u_{i,j+1}^n)\label{eq:2dBe}
	\end{align}
\end{subequations}

First we observe that the truncation errors go like $\mathcal{O}(\Delta t)$, $\mathcal{O}(\Delta x^2)$ and $\mathcal{O}(\Delta y^2)$, so there is no difference in the truncation errors between Forward Euler and Backward Euler, just as for the 1D case.\\

We study stability of the 2D Backward Euler scheme by inserting the 2D ansatz (\ref{eq:ansatz2d}) into
(\ref{eq:2dBe1})

\begin{subequations}
	\begin{align}
	u_{i,j}^n - u_{i,j}^{n-1} 
	&=
	\alpha (u_{i-1,j}^{n} 
	- 4u_{i,j}^n + u_{i+1,j}^n + u_{i,j-1}^n + u_{i,j+1}^n)\\
	a_k - 1 &= \alpha a (e^{-ik \pi \Delta x} + e^{ik \pi \Delta x} -2 + e^{-ik \pi \Delta y} + e^{ik \pi \Delta y} -2 )\\
	&\stackrel{(\ref{eq:naumanFe0})}{=} - 4 \alpha a \Big(\sin^2(k \pi \Delta x/2) +  \sin^2(k \pi \Delta y/2)\Big)\\
	&\stackrel{\Delta x = \Delta y = h}{=} -8 \alpha a \sin^2(kh/2)\\
	a &= \frac{1}{1+ 8 \alpha \sin^2(kh/2)} < 1\label{eq:2dBeStability}
	\end{align}
\end{subequations}

From (\ref{eq:2dBeStability}) we see that the Backward Euler scheme is unconditionally stable in two dimensions, as it was for one dimension. We note that we probably could have applied a simpler ansatz than (\ref{eq:ansatz2d}), since it in (\ref{eq:2dBe1}) was already assumed that $\Delta x = \Delta y$.\\

We see that (\ref{eq:2dBe}) is an implicit scheme, since the only known is $u_{i,j}^n$. (\ref{eq:2dBe}) can be transformed into a linear system of the known type $Au = b$. The procedure for transforming (\ref{eq:2dBe}) into a linear system is the same as demonstrated for the 1D case, see Hjorth-Jensen \cite{MHJ} p. 317. However, there is one important difference compared to the 1D linear system: The matrix is not tridiagonal anymore! This means that we cannot apply the Thomas algorithm anymore, and standard methods as LU-decomposion becomes unpractical. However, in our case the matrix is positive definite, implying that iterative schemes converges to the true solution, Hjorth-Jensen \cite{MHJ} p. 322. Iterative methods are often much faster than direct methods like Gaussian elimination and LU-decomposition. Hence we will solve the implicit problem applying iterative methods. 

\subsubsection{Iterative methods}
This section follows Olver and Shakiban's \cite{olver} chapter on iterative methods closesly. The goal is to present the main idea behind iterative methods, and to present the Jacobi method as part of a general framework.\\

Firstly, an iterative method is not a direct method, like e.g. Gaussian elemination, that solves the problem as it is stated, typically 

\begin{equation}\label{eq:AuB}
	\mathbf{A}\mathbf{u} = \mathbf{b}.
\end{equation} 

Instead, with iterative methods, one attempts solving (\ref{eq:AuB}) by solving another, affine iterative, system:

\begin{equation}\label{eq:iter1}
	\mathbf{u}^{(k+1)} = \mathbf{T} \mathbf{u}^{(k)} + \mathbf{c},
\end{equation}
where $k$ stands for iterations, $T$ is a coefficient matrix of same size as $A$, and $\mathbf{c}$ is a vector. We observe that the right hand side is an affine function of $\mathbf{u}^{(k)}$. For comparison, a linear iterative system is on the form $\mathbf{u}^{(k+1)} = \mathbf{T} \mathbf{u}^{(k)}$, so it is the addition of the vector $\mathbf{c}$ that makes our iterative an affine one, and not a linear one. The affine representation is more genereal compared to the linear one, and allows for more reflexibility in developing iterative schemes.\\

So how is using (\ref{eq:iter1}) in place of solving the original system better? It might not be better. But potentially it can be a lot faster. Look at the FLOP count. In (\ref{eq:iter1}) the matrix-vector multiplication is $\mathcal{O}(N)$ flops, and the addition is of the same order, giving $\mathcal{O}(N)$ FLOPS per iteration. The total FLOP-count for the iterative method is $\mathcal{O}{kN}$ FLOPS. In comparison the Gaussian elimination method is always $\mathcal{O}(N^3)$ FLOPS. This shows that if we can construct a $T$ and $\mathbf{c}$ in our affine iterative scheme (\ref{eq:iter1}) that converges to the true solution after few iterations, the iterative scheme will be much less demanding with respect to FLOPS compared to direct methods like Gaussian elimination.\\

We have shown that the potential of the iterative methods with regards to FLOPS is bug. Now we need to analyse condition for when the method actually works. For the iterative method (\ref{eq:iter1}) to work, it firstly has to converge: 

\begin{equation}\label{eq:itConver}
	\lim_{k \rightarrow \infty} \mathbf{u}^{k} = \mathbf{u}^{*}.
\end{equation}

Taking the limit on both sides of (\ref{eq:iter1}) and using (\ref{eq:itConver}) we get

\begin{subequations}
	\begin{align}
			\lim_{k \rightarrow \infty} \mathbf{u}^{k+1} &= 	\lim_{k \rightarrow \infty} \mathbf{T} \mathbf{u}^k + \mathbf{c}\\
			\mathbf{u}^{*} &= \mathbf{T} \mathbf{u}^{*} + \mathbf{c}.\label{eq:itFixedPoint}
	\end{align}
\end{subequations}

(\ref{eq:itFixedPoint}) is a fixed point equation, and we have that the solution to the iterative scheme has to converge to a fixed point, $\mathbf{u^{*}}$.\\
 
Secondly, the convergent solution $\mathbf{u}^{*}$ that is approached  as $k \rightarrow \infty$ must become equal to the solution $\mathbf{u}$ of the original problem, $\mathbf{A}\mathbf{u} = \mathbf{b}$. If the affine iterative system (\ref{eq:iter1}) converges to a fixed point that differs from the true solution, $\mathbf{u}^{*} \neq \mathbf{u}$, the fixed point splution is of little use for us.\\

Lets now look at conditions for the 1st condition, convergence to the fixed point $\mathbf{u^{*}}$, to hold. We study the error with respect to the fixed point

\begin{subequations}
	\begin{align}
		\mathbf{e}^{k+1} &= \mathbf{u}^{k+1} - \mathbf{u}^{*}\\
		&\stackrel{(\ref{eq:iter1})}{=}  \mathbf{T} \mathbf{u}^k + \mathbf{c} - \mathbf{u}^{*}\\
		&=\mathbf{T} \mathbf{u}^k + \mathbf{c} - (\mathbf{T} \mathbf{u}^{*} + \mathbf{c})\\
		&=\mathbf{T} (\mathbf{u}^k -\mathbf{u}^{*}\\
		&=\mathbf{T}\mathbf{e}^{k}\\
		&=\mathbf{T}^k\mathbf{e}^{(0)}.\label{eq:iterConvError}
	\end{align}
\end{subequations}

(\ref{eq:iterConvError}) shows that the development of the error in convergence towards the fixed point depends on the same matrix as the original iterative problem, $\mathbf{T}$. We have convergence towards the fixed point $\mathbf{u}^{*}$ if $\mathbf{T}$ is a convergent matrix. $\mathbf{T}$ is a convergent matrix if the spectral radius of $\mathbf{T}$ is less than one, $\rho(T) < 1$. The speed of convergence is inversly related to the spectral radius, implying that we want to construct $T$ with as low spectral radius as possible.  \\

We now have a condition for the first requirement for our iterative scheme: convergence to the fixed point. Now for the 2nd condition: convergence to the true solution. It turns out that convergence to the true solution depeneds on the chosen scheme, represented by the matrix $T$ in our case, and the coefficient matrix of the original problem, $A$. We have that both the Jacobi scheme, to be derived, and the Gauss-Seidel scheme are convergent to the true solution if $A$ is strictly diagonally dominant.\\

We sum up our results in this section:\\

The iteration scheme (\ref{eq:iter1}) converges to the solution of the original linear system (\ref{eq:AuB}) if the following two conditions are satisfied

\begin{itemize}
	\item{The spectral radius of $T$ in (\ref{eq:iter1}) is less than one.}
	
	\item{$A$ in (\ref{eq:AuB}) is strictly diagonally dominant (In the case of Jacobi and Gauss-Seidel)}
\end{itemize}

Next we will derive the Jacobi-algorithm and relate it to our genereal iterative system (\ref{eq:iter1}).

\subsubsection{Jacobi's algorithm}
We begin by a rewrite of the linear system (\ref{eq:AuB}), where we split $A$ into one strictly lower triangular matrix, $L$, one strictly upper triangular matrix, $U$, and one diagonal matrix, $D$:

\begin{subequations}
	\begin{align}
		A \mathbf{u} &= \mathbf{b}\\
		(L + D + U) \mathbf{u} &= \mathbf{b}\\
		\mathbf{u} &= D^{-1} \Big(-(L+D) \mathbf{u} + \mathbf{b}\Big)\label{eq:iterJacobi1}
	\end{align}
\end{subequations}

The above is nothing new. It is still the original system. With the Jacobi algorithm, one sets $\mathbf{u}^{k+1} = \mathbf{u}$ on the left hand side of (\ref{eq:iterJacobi1}) and $\mathbf{u} = \mathbf{u}^k$ on the right hand side to get

\begin{subequations}
	\begin{align}
	\mathbf{u}^{k+1}  &= D^{-1} \Big(-(L+D) \mathbf{u}^k + \mathbf{b}\Big)\label{eq:iterJacobi2}
	\end{align}
\end{subequations}

We see that the Jacobi-scheme (\ref{eq:iterJacobi2}) fits our general iterative fixed point scheme (\ref{eq:iter1}), with

\begin{subequations}\label{eq:jacobiFixed}
	\begin{align}
		&T = -D^{-1}(L+D)\\
		&\mathbf{c} = D^{-1} \mathbf{b}.
	\end{align}
\end{subequations}

It is perhaps easier to see the workings of the Jacbi algoritm (\ref{eq:iterJacobi2}) by considering a single row in the matrix-vector system (\ref{eq:iterJacobi2}) 

\begin{subequations}\label{eq:jacobiRow}
	\begin{align}
		u_i^{(k+1)} = - \frac{1}{a_{ii}} \sum_{j=1,i \neq j} a_{ij} u_j^{(k)} + \frac{b_i}{a_{ii}}.
	\end{align}
\end{subequations}

From (\ref{eq:jacobiRow}) we see that only non-diagonal terms are used in the iteration on the right hand side, and that all terms are divided by the diagonal terms from the original matrix $A$. (\ref{eq:jacobiRow}) is very close to the form of the implemented algorithm.\\

Using (\ref{eq:jacobiRow}) we will in the next section see that it is easy to formulate a Jacobi-solver for the 2D implicit scheme (\ref{eq:2dBe}).

\subsubsection{Jacobi's algorithm applied to the 2D implicit diffusion}
Now we will apply Jacobi's algorithm (\ref{eq:jacobiRow}) to the 2D implicit scheme (\ref{eq:2dBe}). We see that (\ref{eq:2dBe}) is already almost set up like a Jacobi-scheme. We get

\begin{subequations}\label{eq:jacobi2D}
	\begin{align}
				u_{i,j}^{n,(k+1)} &= u_{i,j}^{n-1}  + \frac{1}{1+4\alpha}(u_{i-1,j}^{n, (k)} 
		+ u_{i+1,j}^{n,(k)} + u_{i,j-1}^{n,(k)} + u_{i,j+1}^{n,(k)})
	\end{align}
\end{subequations}

(\ref{eq:jacobi2D}) is our Jacobi-Solver. By setting

\begin{subequations}
	\begin{align}
		&a_{ii} = -(1 + 4\alpha)\\
		&b_i = a_{ii} u_{i,j}^{n-1}
	\end{align}
\end{subequations}

in (\ref{eq:jacobi2D}) we see that (\ref{eq:jacobi2D}) corresponds to the Jacobi-algorithm (\ref{eq:jacobiRow}). We also note that only 4 neighboring pints are needed.\\

Following Hjorth-Jensen \cite{MHJ} p. 319, we get the following main algorithm for the Jacobi-solver

\begin{itemize}
	\item{1: Make intial guess for $u_{i,j}$ for all internal points.}
	\item{2: Caclulate iteration $k$ of $u$, $u^k$ using (\ref{eq:2dBe})}.
	\item{3: Stop if wanted convergence reached, else continue next iteration.}
	\item{4: Set previous $u$-value equal to new $u$.}
	\item{5: Repeat all steps from step 2.}
\end{itemize}

The above algorithm is used for all time steps, starting with the first internal time step, solving one step at the time before moving to the next step.

\subsection{2D Physical problem with boundary conditions}
We choose to model a laminar flow inside an infinite tunnel with a finite quadratic cross section. We only model a single cross section, the problem being identical for all cross sections because of the homogenity assumption in the flow direction. \\

There will be no slip boundary conditions on all four walls. The floor and the sides will be fixed, while the roof will be moving in the flow direction. With these boundary conditions we have zero velocity on the floor and the side walls, while we have a non-zero velcocity in the flow direction. The figure below shows our problem.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\textwidth]{/home/karl/doc/subj/att/fys4150/project5/resultsKeep/5e/boxDrawing2-0.png}
	\caption{2D problem sketch}
	\label{1}
\end{figure}

With our choise of coordinate system, positive flow represents flow out of the paper. \\

We choose $u(x,y=1) = 1$, and $u(x=0, y) = u(x=1,y) = u(x,y=0) = 0$. \\

We model the flow with the heat equation. This equation can be derived from the equations of incompressible flow, the Navier-Stokes equations, by assuming laminar flow and a flow profile of type $u = u(x,y)$:

\begin{subequations}
	\begin{align}
		\mathbf{u}_t + (\mathbf{u} \cdot \nabla) \mathbf{u} &= -\frac{\nabla p}{\rho} + \nabla^2 \mathbf{u}\\
		w(x,y)_t + (w \frac{\partial}{\partial z}) w(x,y) &\stackrel{\text{Zero pressure gradient}}{=} (\frac{\partial^2}{\partial x^2} + \frac{\partial^2}{\partial x^2} ) w(x,y)\\
		w(x,y)_t &= (\frac{\partial^2}{\partial x^2} + \frac{\partial^2}{\partial x^2} )w(x,y)\label{eq:ns}
	\end{align}
\end{subequations}

Central assumptions behind the resulting heat equation is that there is no external pressure gradient and that the flow is laminar. 


\begin{thebibliography}{9}
	\bibitem{MHJ} 
	Hjorth-Jensen, M.(2015)
	Computational physics. Lectures fall 2015. 
	\url{https://github.com/CompPhysics/ComputationalPhysics/tree/master/doc/Lectures}
	
	\bibitem{MHJ2}
	\url{https://github.com/CompPhysics/ComputationalPhysics/tree/master/doc/pub/pde}
	
	\bibitem{olver} 
	Olver, P.J and Shakiban, C.(2006)
	Applied linear algebra. Pearson Prentice Hall. 

\end{thebibliography}



\end{document}
